{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bedb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "from perceval.backends.core.git import Git\n",
    "from perceval.backends.core.github import GitHub\n",
    "import requests\n",
    "import pathpy as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbdaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repo = GitHub(owner=\"torvalds\",repository='linux',api_token=[\"<git api token>\"],sleep_for_rate= True)\n",
    "\n",
    "nodeList = []\n",
    "nodeIssueList=[]\n",
    "edgeList = []\n",
    "temporal_edgeList=[]\n",
    "\n",
    "# fetching issues and users from torvalds/linux repo between the interval 9/9/2020 to 10/20/2020\n",
    "for item in repo.fetch(category='issue', from_date=datetime(2020, 9, 9, 0, 0), to_date=datetime(2020,10,20,0,0)):\n",
    "        nodeIssueList.append([item['search_fields']['item_id'],item['data']['number']])\n",
    "        \n",
    "        URL = (item['data']['timeline_url'])\n",
    "        PARAMS={'per_page':'100'}\n",
    "        Headers = {'Authorization': 'token <git api token>'}\n",
    "        #querying timeline URL to fetch various events associated with an issue, to understand user activities.\n",
    "        #user can comment, subscribe, react, push commits etc to an event\n",
    "        r = requests.get(url = URL, params=PARAMS, headers=Headers)\n",
    "        repos=r.json()\n",
    "        while 'next' in r.links.keys():\n",
    "          r=requests.get(r.links['next']['url'])\n",
    "          repos.extend(r.json())\n",
    "        \n",
    "        for data in repos:\n",
    "            \n",
    "            if('created_at' in data):\n",
    "                datetime = parse(data['created_at'],ignoretz=\"True\").isoformat()\n",
    "            if('submitted_at' in data):\n",
    "                datetime = parse(data['submitted_at'],ignoretz=\"True\").isoformat()\n",
    "\n",
    "            #creating 3 types of csv - issues.csv, users.csv, edgeList.csv --> to generate dynamic graph using gephi\n",
    "            #creating csv - temporal_edgelist.csv to generate temporal graph using pathpy \n",
    "            if('user' in data and data['user'] != None):\n",
    "                nodeList.append([data['user']['id'],data['user']['login']])\n",
    "                edgeList.append([data['user']['id'],item['search_fields']['item_id'],'directed',data['event'],datetime,1])\n",
    "                temporal_edgeList.append([data['user']['id'],item['search_fields']['item_id'],datetime])\n",
    "            if('actor' in data and data['actor'] != None):\n",
    "                nodeList.append([data['actor']['id'],data['actor']['login']])\n",
    "                edgeList.append([data['actor']['id'],item['search_fields']['item_id'],'directed',data['event'],datetime,1])\n",
    "                temporal_edgeList.append([data['actor']['id'],item['search_fields']['item_id'],datetime])\n",
    "\n",
    "#remove duplicate users\n",
    "nodeList1 = [i for n, i in enumerate(nodeList) if i not in nodeList[:n]]\n",
    "nodeDF = pd.DataFrame(nodeList1, columns=['id','label'])\n",
    "nodeDF.to_csv('../data/users.csv',sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "nodeIssueListDF = pd.DataFrame(nodeIssueList, columns=['id','label'])\n",
    "nodeIssueListDF.to_csv('../data/issues.csv',sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "edgeDF = pd.DataFrame(edgeList,columns=['source','target','type','label','timeset','weight'])\n",
    "edgeDF.to_csv('../data/edgeList.csv',sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "temporal_edgeDF = pd.DataFrame(temporal_edgeList,columns=['source','target','time'])\n",
    "temporal_edgeDF.to_csv('../data/temporal_edgeList.csv',sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pp.TemporalNetwork.read_file('../data/temporal_edgeList.csv',separator=',', timestamp_format='%Y-%m-%dT%H:%M:%S' ,directed=True, time_rescale=2)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {\n",
    "    # some default parameters\n",
    "    'width': 1200,\n",
    "    'height': 1000,\n",
    "    'ts_per_frame': 50, \n",
    "    'ms_per_frame': 50,\n",
    "    'inactive_edge_width': 4.0,\n",
    "    'active_edge_width': 6.0,\n",
    "    'label_offset': [0,-16],    \n",
    "    'node_size': 10,\n",
    "    'label_size': '14px', \n",
    "}\n",
    "pp.visualisation.plot(t,**style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaca688",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.visualisation.export_html(t, '../images/users_to_issue_temporal_network.html', **style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
